{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f36d6a4",
   "metadata": {},
   "source": [
    "df_physicochemical comprises the selected features phychem_features and the whole set of parameters if necesary for plotting purposes.\n",
    "The df_micro is the df of protein-genus features selected from the notebook 7_visual_proteins_ipnyb\n",
    "micro_usuals is a dictionary with the list of proven bacteria influencing corrosion and could serve as label for plotting purposes\n",
    "micro_markers is the dictionary with the list of bacteria belonging to the df_micro dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57d8e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.12/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_19772/823879769.py\", line 17, in <module>\n",
      "    import torch\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c19f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Local\n",
    "base_dir = Path(\"/home/beatriz/MIC/3_combined/data\")\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "markers_path = base_dir /\"combined_markers.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf64e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local (VSCode) environment\n"
     ]
    }
   ],
   "source": [
    "# Deciding which environment\n",
    "if Path(\"/kaggle\").exists():\n",
    "    print(\"Running in Kaggle environment\")\n",
    "    # For Kaggle work# Input datasets (read-only in Kaggle) # Files in small input directory\n",
    "    base_dir = Path(\"/kaggle/input\")  \n",
    "    abundance_excel = base_dir / \"new-picrust/merged_to_sequence.xlsx\" # inside input small sizes input\n",
    "    #Input market groups\n",
    "    market_dir = base_dir / \"markers\"\n",
    "    # Output dirs\n",
    "    output_base = Path(\"/kaggle/working/\")\n",
    "    # Save the present working data\n",
    "    combined_path = output_base /\"combined_markers.xlsx\"\n",
    "    #Directory to keep  Results\n",
    "    share_dir = output_base/\"Visualisations\"\n",
    "    shared_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "else:\n",
    "    print(\"Running in local (VSCode) environment\")\n",
    "    base_dir = Path(\"data\")\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Base Paths for local environment\n",
    "    abundance_excel = base_dir / \"merged_to_sequence.xlsx\"\n",
    "    # This files are too large for github and are store on Kaggle for educational purposes\n",
    "    output_large = Path(\"/home/beatriz/MIC/output_large\")\n",
    "    #Input market groups\n",
    "    market_dir = output_large / \"markers.parquet/\"  # Directory\n",
    "    output_base = base_dir \n",
    "    #Directory to keep some Results\n",
    "    shared_dir= Path(\"/home/beatriz/SharedFolder/Visualisations/\")\n",
    "    combined_path = base_dir / \"combined_markers.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc808b5",
   "metadata": {},
   "source": [
    "### Importing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf81ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_df= pd.read_excel(markers_path, sheet_name='protein_markers',  engine ='openpyxl')\n",
    "all_physichem = pd.read_excel(markers_path, sheet_name='all_physicochemical', engine ='openpyxl')\n",
    "genus_site_df = pd.read_excel(markers_path, sheet_name='genus_to_sites', engine ='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31bd6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "physichem_features = ['Sites', 'Label', 'Temperature', 'Type', 'EC_M', 'O2_Eh',\n",
    "                     'Ox_Fe_Zn', 'Cl_SO4_NO3', 'Na_K','pH_HPO4',\n",
    "                       'Ca_HCO3_Mg', 'Cu_Al_Mn', 'Ni_Cr_Mo']\n",
    "physichem_df = all_physichem[physichem_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8fa97",
   "metadata": {},
   "source": [
    "As a first approximation I will take the first 20 with best score, but I will have to iterate through the list and check if these proteins are really relevant to corrosion states or are also found on the other part of the community the other 800 bacteria and if so what is the numerical diffence, if as I think these are universal proteins and belong infact also to the other part of the community, then we have to discard them and found the ones that really make a difference on the failed systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913c83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_df = micro_df.sort_values(by='score_combined', ascending=False).head(20)\n",
    "micro_df = micro_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4e9a1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exploded_df = exploded_df.explode(\\'genera\\')\\nexploded_df = exploded_df.rename(columns={\"genera\": \"Genus\"})\\n\\nexploded_df = exploded_df.drop(\\'Unnamed: 0\\', axis=1)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If they're strings that look like lists, we need to convert them to actual lists\n",
    "import ast\n",
    "\n",
    "# Function to safely convert string representations of lists to actual lists\n",
    "def convert_str_to_list(value):\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return value\n",
    "    return value\n",
    "\n",
    "# Convert string lists to actual lists because otherwise the expansion wont work\n",
    "if isinstance(micro_df['genera'].iloc[0], str) and '[' in micro_df['genera'].iloc[0]:\n",
    "    micro_df['genera'] = micro_df['genera'].apply(convert_str_to_list)\n",
    "    \n",
    "if isinstance(micro_df['Sites'].iloc[0], str) and '[' in micro_df['Sites'].iloc[0]:\n",
    "    micro_df['Sites'] = micro_df['Sites'].apply(convert_str_to_list)\n",
    "\n",
    "# exploding sites \n",
    "exploded_df = micro_df.explode('Sites')\n",
    "exploded_df = exploded_df.set_index(\"Sites\").sort_index()\n",
    "exploded_df = exploded_df.reset_index()\n",
    "'''exploded_df = exploded_df.explode('genera')\n",
    "exploded_df = exploded_df.rename(columns={\"genera\": \"Genus\"})\n",
    "\n",
    "exploded_df = exploded_df.drop('Unnamed: 0', axis=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fd40591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification results:\n",
      "valid_relationship\n",
      "False    1740\n",
      "True      186\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "micro_features = exploded_df.merge(genus_site_df[[\"Sites\", \"Genus\"]], \n",
    "    on='Sites',  \n",
    "    how='inner'\n",
    ")\n",
    "# are a subset of the genera in the original hierarchy_df for each protein\n",
    "def verify_join(row):\n",
    "    protein = row['protein_name']\n",
    "    genus = row['Genus']\n",
    "    original_genera = micro_df[micro_df['protein_name'] == protein]['genera'].iloc[0]\n",
    "    return genus in original_genera\n",
    "\n",
    "# Add a verification column\n",
    "micro_features['valid_relationship'] = micro_features.apply(verify_join, axis=1)\n",
    "\n",
    "# Check how many rows have valid relationships\n",
    "print(\"\\nVerification results:\")\n",
    "print(micro_features['valid_relationship'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fcc1984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>protein_name</th>\n",
       "      <th>genera_count</th>\n",
       "      <th>genera</th>\n",
       "      <th>functional_categories</th>\n",
       "      <th>niche_pathways</th>\n",
       "      <th>enzyme_class</th>\n",
       "      <th>corrosion_mechanisms</th>\n",
       "      <th>score_combined</th>\n",
       "      <th>Sites</th>\n",
       "      <th>Genus</th>\n",
       "      <th>valid_relationship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>ferredoxin---nad+ reductase; ferredoxin-nicoti...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Acidisoma, Pseudorhodoferax, Methylocystis, S...</td>\n",
       "      <td>['electron transfer &amp; redox', 'o2_consumption'...</td>\n",
       "      <td>['fluorobenzoate degradation', 'polycyclic aro...</td>\n",
       "      <td>Acting on iron-sulfur proteins as donors.</td>\n",
       "      <td>['direct_eet', 'o2_consumption', 'indirect_eet...</td>\n",
       "      <td>53.040891</td>\n",
       "      <td>site_8</td>\n",
       "      <td>Azospira</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>ferredoxin---nad+ reductase; ferredoxin-nicoti...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Acidisoma, Pseudorhodoferax, Methylocystis, S...</td>\n",
       "      <td>['electron transfer &amp; redox', 'o2_consumption'...</td>\n",
       "      <td>['fluorobenzoate degradation', 'polycyclic aro...</td>\n",
       "      <td>Acting on iron-sulfur proteins as donors.</td>\n",
       "      <td>['direct_eet', 'o2_consumption', 'indirect_eet...</td>\n",
       "      <td>53.040891</td>\n",
       "      <td>site_8</td>\n",
       "      <td>Legionella</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>ferredoxin---nad+ reductase; ferredoxin-nicoti...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Acidisoma, Pseudorhodoferax, Methylocystis, S...</td>\n",
       "      <td>['electron transfer &amp; redox', 'o2_consumption'...</td>\n",
       "      <td>['fluorobenzoate degradation', 'polycyclic aro...</td>\n",
       "      <td>Acting on iron-sulfur proteins as donors.</td>\n",
       "      <td>['direct_eet', 'o2_consumption', 'indirect_eet...</td>\n",
       "      <td>53.040891</td>\n",
       "      <td>site_8</td>\n",
       "      <td>Novosphingobium</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>ferredoxin---nad+ reductase; ferredoxin-nicoti...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Acidisoma, Pseudorhodoferax, Methylocystis, S...</td>\n",
       "      <td>['electron transfer &amp; redox', 'o2_consumption'...</td>\n",
       "      <td>['fluorobenzoate degradation', 'polycyclic aro...</td>\n",
       "      <td>Acting on iron-sulfur proteins as donors.</td>\n",
       "      <td>['direct_eet', 'o2_consumption', 'indirect_eet...</td>\n",
       "      <td>53.040891</td>\n",
       "      <td>site_8</td>\n",
       "      <td>Opitutus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>ferredoxin---nad+ reductase; ferredoxin-nicoti...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Acidisoma, Pseudorhodoferax, Methylocystis, S...</td>\n",
       "      <td>['electron transfer &amp; redox', 'o2_consumption'...</td>\n",
       "      <td>['fluorobenzoate degradation', 'polycyclic aro...</td>\n",
       "      <td>Acting on iron-sulfur proteins as donors.</td>\n",
       "      <td>['direct_eet', 'o2_consumption', 'indirect_eet...</td>\n",
       "      <td>53.040891</td>\n",
       "      <td>site_8</td>\n",
       "      <td>Acidisoma</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       protein_name  \\\n",
       "0          16  ferredoxin---nad+ reductase; ferredoxin-nicoti...   \n",
       "1          16  ferredoxin---nad+ reductase; ferredoxin-nicoti...   \n",
       "2          16  ferredoxin---nad+ reductase; ferredoxin-nicoti...   \n",
       "3          16  ferredoxin---nad+ reductase; ferredoxin-nicoti...   \n",
       "4          16  ferredoxin---nad+ reductase; ferredoxin-nicoti...   \n",
       "\n",
       "   genera_count                                             genera  \\\n",
       "0             4  [Acidisoma, Pseudorhodoferax, Methylocystis, S...   \n",
       "1             4  [Acidisoma, Pseudorhodoferax, Methylocystis, S...   \n",
       "2             4  [Acidisoma, Pseudorhodoferax, Methylocystis, S...   \n",
       "3             4  [Acidisoma, Pseudorhodoferax, Methylocystis, S...   \n",
       "4             4  [Acidisoma, Pseudorhodoferax, Methylocystis, S...   \n",
       "\n",
       "                               functional_categories  \\\n",
       "0  ['electron transfer & redox', 'o2_consumption'...   \n",
       "1  ['electron transfer & redox', 'o2_consumption'...   \n",
       "2  ['electron transfer & redox', 'o2_consumption'...   \n",
       "3  ['electron transfer & redox', 'o2_consumption'...   \n",
       "4  ['electron transfer & redox', 'o2_consumption'...   \n",
       "\n",
       "                                      niche_pathways  \\\n",
       "0  ['fluorobenzoate degradation', 'polycyclic aro...   \n",
       "1  ['fluorobenzoate degradation', 'polycyclic aro...   \n",
       "2  ['fluorobenzoate degradation', 'polycyclic aro...   \n",
       "3  ['fluorobenzoate degradation', 'polycyclic aro...   \n",
       "4  ['fluorobenzoate degradation', 'polycyclic aro...   \n",
       "\n",
       "                                enzyme_class  \\\n",
       "0  Acting on iron-sulfur proteins as donors.   \n",
       "1  Acting on iron-sulfur proteins as donors.   \n",
       "2  Acting on iron-sulfur proteins as donors.   \n",
       "3  Acting on iron-sulfur proteins as donors.   \n",
       "4  Acting on iron-sulfur proteins as donors.   \n",
       "\n",
       "                                corrosion_mechanisms  score_combined   Sites  \\\n",
       "0  ['direct_eet', 'o2_consumption', 'indirect_eet...       53.040891  site_8   \n",
       "1  ['direct_eet', 'o2_consumption', 'indirect_eet...       53.040891  site_8   \n",
       "2  ['direct_eet', 'o2_consumption', 'indirect_eet...       53.040891  site_8   \n",
       "3  ['direct_eet', 'o2_consumption', 'indirect_eet...       53.040891  site_8   \n",
       "4  ['direct_eet', 'o2_consumption', 'indirect_eet...       53.040891  site_8   \n",
       "\n",
       "             Genus  valid_relationship  \n",
       "0         Azospira               False  \n",
       "1       Legionella               False  \n",
       "2  Novosphingobium               False  \n",
       "3         Opitutus               False  \n",
       "4        Acidisoma                True  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a00f7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'site_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m corr = \u001b[43mdf_combined_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[top_features].loc[\u001b[33m\"\u001b[39m\u001b[33mO2_Eh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpH_PO4\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mFe_Zn_Ox\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MIC/3_combined/.venv/lib/python3.12/site-packages/pandas/core/frame.py:11049\u001b[39m, in \u001b[36mDataFrame.corr\u001b[39m\u001b[34m(self, method, min_periods, numeric_only)\u001b[39m\n\u001b[32m  11047\u001b[39m cols = data.columns\n\u001b[32m  11048\u001b[39m idx = cols.copy()\n\u001b[32m> \u001b[39m\u001b[32m11049\u001b[39m mat = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m  11051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mpearson\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  11052\u001b[39m     correl = libalgos.nancorr(mat, minp=min_periods)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MIC/3_combined/.venv/lib/python3.12/site-packages/pandas/core/frame.py:1993\u001b[39m, in \u001b[36mDataFrame.to_numpy\u001b[39m\u001b[34m(self, dtype, copy, na_value)\u001b[39m\n\u001b[32m   1991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1992\u001b[39m     dtype = np.dtype(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1993\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[32m   1995\u001b[39m     result = np.asarray(result, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MIC/3_combined/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:1694\u001b[39m, in \u001b[36mBlockManager.as_array\u001b[39m\u001b[34m(self, dtype, copy, na_value)\u001b[39m\n\u001b[32m   1692\u001b[39m         arr.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m     arr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[32m   1696\u001b[39m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MIC/3_combined/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:1753\u001b[39m, in \u001b[36mBlockManager._interleave\u001b[39m\u001b[34m(self, dtype, na_value)\u001b[39m\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1752\u001b[39m         arr = blk.get_values(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1753\u001b[39m     \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m = arr\n\u001b[32m   1754\u001b[39m     itemmask[rl.indexer] = \u001b[32m1\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask.all():\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'site_1'"
     ]
    }
   ],
   "source": [
    "corr = df_combined_features.corr()[top_features].loc[\"O2_Eh\", \"pH_PO4\", \"Fe_Zn_Ox\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a654e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6e9e3f0",
   "metadata": {},
   "source": [
    "## Data Splitting Strategy\n",
    "\n",
    "To ensure robust feature engineering and prevent data leakage, a portion of the data will be reserved for final model validation. The split must preserve the distribution of key factors affecting corrosion mechanisms:\n",
    "\n",
    "1. **Corrosion Severity Label (Primary)** - Ensures balanced representation of corrosion levels and Essential for model evaluation across all severity classes\n",
    "\n",
    "2. **Material Composition**: Different materials exhibit distinct corrosion mechanisms\n",
    "\n",
    "3. **System Temperature Regime** Hot/Cold/Combined systems affect: Reaction kinetics, oxygen solubility, protective film formation, mineral precipitation tendencies\n",
    "\n",
    "4. **Geographical Location**: Influences water chemistry through<. Different treatment regulations (chlorine vs. non-chlorine), regional geological variations in mineral content, country-specific water quality standards\n",
    "\n",
    "5. **System Age (Secondary)**: Collection period: 2014-2018, while potentially relevant for corrosion progression it is considered less critical due to varying maintenance histories and treatment variations make precise temporal effects difficult to isolate\n",
    "\n",
    "This stratified splitting approach ensures the test set remains representative while maintaining the independence necessary for valid model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_material_group(df, material_column='Material'):\n",
    "    \"\"\"\n",
    "    Groups materials based on cluster analysis findings and material properties.\n",
    "    \n",
    "    Key groupings:\n",
    "    - Steel_group: Combines Stainless_Steel and Steel\n",
    "    - GSP_group: Combines Galvanized_Steel and Galvanized_Steel_Plastic\n",
    "    - Keeps Galvanized_Carbon_Steel separate due to distinct cluster behavior\n",
    "    - Other materials remain separate for individual analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame containing material information\n",
    "    material_column : str, default='Material'\n",
    "        Name of the column containing material names\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with added 'material_group' column\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the material column to avoid modifying the original\n",
    "    materials = df[material_column].copy()\n",
    "    \n",
    "    grouped_materials = []\n",
    "    \n",
    "    for material in materials:\n",
    "        # Standardize material name\n",
    "        material_strip = str(material).strip()\n",
    "        \n",
    "        # Group 1: Combine Stainless Steel and Steel\n",
    "        if material_strip in ['Stainless_Steel', 'Steel']:\n",
    "            grouped_materials.append('Steel_group')\n",
    "        \n",
    "        # Group 2: Combine Galvanized Steel and GSP\n",
    "        elif material_strip in ['Galvanized_Steel', 'Galvanized_Steel_Plastic']:\n",
    "            grouped_materials.append('GS_group')\n",
    "        \n",
    "        # Keep Galvanized Carbon Steel separate\n",
    "        elif material_strip in ['Galvanized_Carbon_Steel', 'Galvanized_carbon_steel_plastic']:\n",
    "            grouped_materials.append('GCS_group')\n",
    "    \n",
    "        # Keep Cooper as is\n",
    "        elif material_strip == 'Cooper':\n",
    "            grouped_materials.append('Cooper_group')\n",
    "        \n",
    "        # Keep Cross-Linked Polyethylene and THSP as others\n",
    "        elif material_strip in ['Cross-Linked_Polyethylene','Tinplate_High-Strength_Plastic']:\n",
    "            grouped_materials.append('CLP_THSP_group')\n",
    "           \n",
    "    # Add the grouped materials as a new column\n",
    "    df['material_group'] = grouped_materials\n",
    "    return df\n",
    "\n",
    "#the indexes will be kept same as the whole df\n",
    "def create_stratification_groups_v2(row):\n",
    "    \"\"\"\n",
    "    Create comprehensive stratification groups\n",
    "    \"\"\"\n",
    "    # Water treatment regime based on country\n",
    "    water_regime = 'chlorine' if row['Country'] in ['Belgium', 'Netherlands'] else 'no_chlorine' \n",
    "    \n",
    "    # Create stratification group string\n",
    "    strat_group = f\"{row['material_group']}_{water_regime}_label{row['Label']}\" # _{row['Type']}_ I removed the Type condition because it divides the samples in too many classes. \n",
    "    \n",
    "    return strat_group  \n",
    "\n",
    "def split_dataset_v2(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset while maintaining distributions of key features and original indeces\n",
    "    \"\"\"\n",
    "    #Creating a split column with default value, so the spliting can be applied to other dataframe\n",
    "    df['split'] = 'train' # as default value\n",
    "    \n",
    "    # Add material group column\n",
    "    df = get_material_group(df)\n",
    "    #reindexing \n",
    "    df.index = original_indices\n",
    "    # Create stratification groups\n",
    "    df['strat_group'] = df.apply(create_stratification_groups_v2, axis=1)\n",
    "    \n",
    "    # Identify groups with sufficient samples\n",
    "    group_counts = df['strat_group'].value_counts()\n",
    "    large_groups = group_counts[group_counts >= 4].index\n",
    "    small_groups = group_counts[group_counts < 4].index\n",
    "    \n",
    "    # Split data based on group size\n",
    "    large_data = df[df['strat_group'].isin(large_groups)]\n",
    "    small_data = df[df['strat_group'].isin(small_groups)]\n",
    "    \n",
    "    if len(large_data) > 0:\n",
    "        # Stratified split for large groups\n",
    "        train_large_idx, test_large_idx = train_test_split(\n",
    "            large_data.index,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=large_data['strat_group']\n",
    "        )\n",
    "        \n",
    "        if len(small_data) > 0:\n",
    "            train_small_idx, test_small_idx = train_test_split(\n",
    "                small_data.index,\n",
    "                test_size=test_size,\n",
    "                random_state=random_state,\n",
    "                stratify=small_data['strat_group']\n",
    "            )\n",
    "            # Combine indices\n",
    "            train_idx = np.concatenate([train_large_idx, train_small_idx])\n",
    "            test_idx = np.concatenate([test_large_idx, test_small_idx])\n",
    "            \n",
    "        else:  # This else belongs to the small_data if\n",
    "            train_idx = train_large_idx\n",
    "            test_idx = test_large_idx\n",
    "    else:  # This else belongs to the large_data if\n",
    "        # Simple split if no large groups\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            df.index,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    \n",
    "    # Mark split in original dataframe\n",
    "    df.loc[test_idx, 'split'] = 'test'\n",
    "    \n",
    "    # Return train and test dataframes with original indices\n",
    "    return df[df['split'] == 'train'], df[df['split'] == 'test']\n",
    "    \n",
    "def analyze_split_results(train_df, test_df, original_df):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of features in the split datasets\n",
    "    \"\"\"\n",
    "    print(\"=== Split Size Analysis ===\")\n",
    "    print(f\"Total samples: {len(original_df)}\")\n",
    "    print(f\"Training samples: {len(train_df)} ({len(train_df)/len(original_df)*100:.1f}%)\")\n",
    "    print(f\"Test samples: {len(test_df)} ({len(test_df)/len(original_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze distributions\n",
    "    features = ['Material', 'Country', 'Label']\n",
    "    \n",
    "    for feature in features:\n",
    "        print(f\"\\n=== {feature} Distribution ===\")\n",
    "        \n",
    "        # Calculate distributions\n",
    "        train_dist = train_df[feature].value_counts(normalize=True)\n",
    "        test_dist = test_df[feature].value_counts(normalize=True)\n",
    "        original_dist = original_df[feature].value_counts(normalize=True)\n",
    "        \n",
    "        # Combine into a DataFrame\n",
    "        dist_df = pd.DataFrame({\n",
    "            'Original %': original_dist * 100,\n",
    "            'Train %': train_dist * 100,\n",
    "            'Test %': test_dist * 100,\n",
    "            'Original Count': original_df[feature].value_counts(),\n",
    "            'Train Count': train_df[feature].value_counts(),\n",
    "            'Test Count': test_df[feature].value_counts()\n",
    "        }).round(2)\n",
    "        \n",
    "        print(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d6404",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_Meta_Split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m grouped_materials_df = get_material_group(\u001b[43mdf_Meta_Split\u001b[49m, material_column=\u001b[33m'\u001b[39m\u001b[33mMaterial\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Apply the split\u001b[39;00m\n\u001b[32m      3\u001b[39m train_df, test_df = split_dataset_v2(grouped_materials_df)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_Meta_Split' is not defined"
     ]
    }
   ],
   "source": [
    "grouped_materials_df = get_material_group(df_Meta_Split, material_column='Material')\n",
    "# Apply the split\n",
    "train_df, test_df = split_dataset_v2(grouped_materials_df)\n",
    "# Analyzing the results\n",
    "analyze_split_results(train_df, test_df, grouped_materials_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c06d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now applying the split to original dataframe\n",
    "split_mapping = grouped_materials_df['split']\n",
    "original['split'] = original.index.map(split_mapping)\n",
    "\n",
    "# Get train and test sets for original dataframe\n",
    "original_train = original[original['split'] == 'train']\n",
    "original_test = original[original['split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify indices match\n",
    "print(\"Train indices match:\", set(train_df.index) == set(original_train.index))\n",
    "print(\"Test indices match:\", set(test_df.index) == set(original_test.index))\n",
    "print(\"No overlap between train and test:\", len(set(train_df.index) & set(test_df.index)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we assigne the original_train set to the dataframe to work in this notebook moving forward and we keep the original_test df to work for the model validation\n",
    "df = original_train.drop(columns=['split']).copy() # .drop(columns=['Label']).values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a5d80",
   "metadata": {},
   "source": [
    "The initial strategy for dataset partitioning aimed to ensure a balanced representation across Location, Material, Type, and Label variables. Despite implementing sophisticated preprocessing steps - including hierarchical cluster analysis for material grouping and stratification of locations based on Cl- treatment protocols - the limited sample size (n=13) proved insufficient relative to the number of distinct classes, preventing a statistically valid split.\n",
    "\n",
    "To address this limitation, Type was removed from the stratification criteria. This decision was supported by two key analytical findings:\n",
    "\n",
    "Principal Component Analysis (PCA) in three dimensions demonstrated strong clustering patterns based on materials and locations alone, suggesting these features effectively capture the underlying data structure.\n",
    "\n",
    "Feature importance analysis using XGBoost revealed that Composition accounts for approximately 50% of the variance in material type distribution across clusters, indicating that material properties are inherently captured through compositional data.\n",
    "\n",
    "This modification to the stratification approach maintains the essential patterns in the data while enabling a more robust train-test split for subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel = df[top_features]\n",
    "y = df['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # or softmax for >2 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cef101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.12/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_37861/169521571.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Data preparation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X_sel = \u001b[43mdf\u001b[49m[top_features]\n\u001b[32m     10\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mCategory\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data preparation\n",
    "X_sel = df[top_features]\n",
    "y = df['Category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values)\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 32)\n",
    "        self.layer2 = nn.Linear(32, 16)\n",
    "        self.layer3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "model = SimpleNN(input_size)\n",
    "print(f\"Model created with input size: {input_size}\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with smaller batches\n",
    "num_epochs = 10\n",
    "batch_size = 8  # Small batch size for memory efficiency\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Use smaller batches to reduce memory usage\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Get batch\n",
    "        batch_X = X_train_tensor[i:i+batch_size]\n",
    "        batch_y = y_train_tensor[i:i+batch_size].view(-1, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / (len(X_train) / batch_size)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Evaluate every epoch to track progress\n",
    "    if (epoch + 1) % 2 == 0:  # Check every 2 epochs\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test_tensor)\n",
    "            y_pred_class = (y_pred > 0.5).float()\n",
    "            accuracy = (y_pred_class.view(-1) == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "            print(f'Validation Accuracy after epoch {epoch+1}: {accuracy:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_class = (y_pred > 0.5).float()\n",
    "    accuracy = (y_pred_class.view(-1) == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "    print(f'Final Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Print predictions for first few samples\n",
    "    for i in range(min(5, len(y_test))):\n",
    "        print(f\"Sample {i+1}: Actual: {y_test.iloc[i]}, Predicted: {y_pred[i].item():.4f}, Class: {y_pred_class[i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3_Combined Environment",
   "language": "python",
   "name": "combined_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
