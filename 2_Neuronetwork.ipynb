{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f36d6a4",
   "metadata": {},
   "source": [
    "df_physicochemical comprises the selected features phychem_features and the whole set of parameters if necesary for plotting purposes.\n",
    "The df_micro is the df of protein-genus features selected from the notebook 7_visual_proteins_ipnyb\n",
    "micro_usuals is a dictionary with the list of proven bacteria influencing corrosion and could serve as label for plotting purposes\n",
    "micro_markers is the dictionary with the list of bacteria belonging to the df_micro dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57d8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c19f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Local\n",
    "base_dir = Path(\"/home/beatriz/MIC/3_combined/data\")\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "markers_path = base_dir /\"combined_markers.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf64e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local (VSCode) environment\n"
     ]
    }
   ],
   "source": [
    "# Deciding which environment\n",
    "if Path(\"/kaggle\").exists():\n",
    "    print(\"Running in Kaggle environment\")\n",
    "    # For Kaggle work# Input datasets (read-only in Kaggle) # Files in small input directory\n",
    "    base_dir = Path(\"/kaggle/input\")  \n",
    "    abundance_excel = base_dir / \"new-picrust/merged_to_sequence.xlsx\" # inside input small sizes input\n",
    "    #Input market groups\n",
    "    market_dir = base_dir / \"markers\"\n",
    "    # Output dirs\n",
    "    output_base = Path(\"/kaggle/working/\")\n",
    "    # Save the present working data\n",
    "    combined_path = output_base /\"combined_markers.xlsx\"\n",
    "    #Directory to keep  Results\n",
    "    share_dir = output_base/\"Visualisations\"\n",
    "    shared_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "else:\n",
    "    print(\"Running in local (VSCode) environment\")\n",
    "    base_dir = Path(\"data\")\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Base Paths for local environment\n",
    "    abundance_excel = base_dir / \"merged_to_sequence.xlsx\"\n",
    "    # This files are too large for github and are store on Kaggle for educational purposes\n",
    "    output_large = Path(\"/home/beatriz/MIC/output_large\")\n",
    "    #Input market groups\n",
    "    market_dir = output_large / \"markers.parquet/\"  # Directory\n",
    "    output_base = base_dir \n",
    "    #Directory to keep some Results\n",
    "    shared_dir= Path(\"/home/beatriz/SharedFolder/Visualisations/\")\n",
    "    combined_path = base_dir / \"combined_markers.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc808b5",
   "metadata": {},
   "source": [
    "### Importing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baf81ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_df= pd.read_excel(markers_path, sheet_name='protein_markers',  engine ='openpyxl')\n",
    "all_physichem = pd.read_excel(markers_path, sheet_name='all_physicochemical', engine ='openpyxl')\n",
    "genus_site_mapping = pd.read_excel(markers_path, sheet_name='genus_to_sites', engine ='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31bd6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "physichem_features = ['Sites', 'Label', 'Temperature', 'Type', 'EC_M', 'O2_Eh',\n",
    "                     'Ox_Fe_Zn', 'Cl_SO4_NO3', 'Na_K','pH_HPO4',\n",
    "                       'Ca_HCO3_Mg', 'Cu_Al_Mn', 'Ni_Cr_Mo']\n",
    "physichem_df = all_physichem[physichem_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "913c83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_df = micro_markers.sort_values(by='score_combined', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef860b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, add the Sites column based on genera\n",
    "def map_genus_to_sites(genus):\n",
    "    # Return sites for a single genus (not a list of genera)\n",
    "    return genus_site_mapping.get(genus, [])\n",
    "\n",
    "# Apply the function to each genus individually\n",
    "# First, explode the genera column to have one row per protein-genus pair\n",
    "exploded_by_genus = micro_df.explode('genera')\n",
    "\n",
    "# Now add the sites for each genus\n",
    "exploded_by_genus['Sites'] = exploded_by_genus['genera'].apply(map_genus_to_sites)\n",
    "\n",
    "# Explode again to have one row per protein-genus-site combination\n",
    "fully_exploded_df = exploded_by_genus.explode('Sites')\n",
    "\n",
    "# Remove rows where Sites is None or empty\n",
    "#fully_exploded_df = fully_exploded_df[fully_exploded_df['Sites'].notna() & (fully_exploded_df['Sites'] != '')]\n",
    "\n",
    "# Now join with physichem_df\n",
    "#result_df = fully_exploded_df.merge(physichem_df, on='Sites', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d545e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>protein_name</th>\n",
       "      <th>genera_count</th>\n",
       "      <th>genera</th>\n",
       "      <th>functional_categories</th>\n",
       "      <th>niche_pathways</th>\n",
       "      <th>enzyme_class</th>\n",
       "      <th>corrosion_mechanisms</th>\n",
       "      <th>score_combined</th>\n",
       "      <th>Sites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16</td>\n",
       "      <td>ferredoxin---nad+ reductase; ferredoxin-nicoti...</td>\n",
       "      <td>4</td>\n",
       "      <td>['Acidisoma', 'Pseudorhodoferax', 'Methylocyst...</td>\n",
       "      <td>['h2_consumption', 'iron/sulfur_redox', 'acid_...</td>\n",
       "      <td>['xylene degradation', 'dioxin degradation', '...</td>\n",
       "      <td>Acting on iron-sulfur proteins as donors.</td>\n",
       "      <td>['h2_consumption', 'direct_eet', 'sulfur_metab...</td>\n",
       "      <td>53.040891</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>34</td>\n",
       "      <td>enoyl-[ reductase (nadh); enoyl-[ reductase; e...</td>\n",
       "      <td>3</td>\n",
       "      <td>['Aestuariimicrobium', 'Thermincola', 'Trepone...</td>\n",
       "      <td>['h2_consumption', 'iron/sulfur_redox', 'acid_...</td>\n",
       "      <td>['lipid biosynthesis proteins', 'fatty acid bi...</td>\n",
       "      <td>Acting on the CH-CH group of donors.</td>\n",
       "      <td>['h2_consumption', 'direct_eet', 'sulfur_metab...</td>\n",
       "      <td>51.298188</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>enoyl-[ reductase [ (ec 1.3.1.9)</td>\n",
       "      <td>11</td>\n",
       "      <td>['Gallionella', 'Acidisoma', 'Mycoplana', 'Ano...</td>\n",
       "      <td>['h2_consumption', 'iron/sulfur_redox', 'acid_...</td>\n",
       "      <td>['lipid biosynthesis proteins', 'fatty acid bi...</td>\n",
       "      <td>Acting on the CH-CH group of donors.</td>\n",
       "      <td>['h2_consumption', 'direct_eet', 'sulfur_metab...</td>\n",
       "      <td>50.718367</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>26</td>\n",
       "      <td>siroheme-synthase [</td>\n",
       "      <td>3</td>\n",
       "      <td>['Gallionella', 'Pseudomonas', 'Thiobacillus']</td>\n",
       "      <td>['h2_consumption', 'iron/sulfur_redox', 'acid_...</td>\n",
       "      <td>['nitrogen , nitrogen cycle']</td>\n",
       "      <td>Acting on other nitrogenous compounds as donors.</td>\n",
       "      <td>['h2_consumption', 'direct_eet', 'nitrogen_met...</td>\n",
       "      <td>50.402663</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>enoyl-[ reductase (nadph, si-specific); acyl-a...</td>\n",
       "      <td>10</td>\n",
       "      <td>['Gallionella', 'Acidisoma', 'Aestuariimicrobi...</td>\n",
       "      <td>['h2_consumption', 'iron/sulfur_redox', 'acid_...</td>\n",
       "      <td>['biotin', 'fatty acid biosynthesis', 'lipid b...</td>\n",
       "      <td>Acting on the CH-CH group of donors.</td>\n",
       "      <td>['h2_consumption', 'direct_eet', 'sulfur_metab...</td>\n",
       "      <td>50.379373</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                       protein_name  \\\n",
       "20          16  ferredoxin---nad+ reductase; ferredoxin-nicoti...   \n",
       "24          34  enoyl-[ reductase (nadh); enoyl-[ reductase; e...   \n",
       "1            1                   enoyl-[ reductase [ (ec 1.3.1.9)   \n",
       "35          26                                siroheme-synthase [   \n",
       "2            2  enoyl-[ reductase (nadph, si-specific); acyl-a...   \n",
       "\n",
       "    genera_count                                             genera  \\\n",
       "20             4  ['Acidisoma', 'Pseudorhodoferax', 'Methylocyst...   \n",
       "24             3  ['Aestuariimicrobium', 'Thermincola', 'Trepone...   \n",
       "1             11  ['Gallionella', 'Acidisoma', 'Mycoplana', 'Ano...   \n",
       "35             3     ['Gallionella', 'Pseudomonas', 'Thiobacillus']   \n",
       "2             10  ['Gallionella', 'Acidisoma', 'Aestuariimicrobi...   \n",
       "\n",
       "                                functional_categories  \\\n",
       "20  ['h2_consumption', 'iron/sulfur_redox', 'acid_...   \n",
       "24  ['h2_consumption', 'iron/sulfur_redox', 'acid_...   \n",
       "1   ['h2_consumption', 'iron/sulfur_redox', 'acid_...   \n",
       "35  ['h2_consumption', 'iron/sulfur_redox', 'acid_...   \n",
       "2   ['h2_consumption', 'iron/sulfur_redox', 'acid_...   \n",
       "\n",
       "                                       niche_pathways  \\\n",
       "20  ['xylene degradation', 'dioxin degradation', '...   \n",
       "24  ['lipid biosynthesis proteins', 'fatty acid bi...   \n",
       "1   ['lipid biosynthesis proteins', 'fatty acid bi...   \n",
       "35                      ['nitrogen , nitrogen cycle']   \n",
       "2   ['biotin', 'fatty acid biosynthesis', 'lipid b...   \n",
       "\n",
       "                                        enzyme_class  \\\n",
       "20         Acting on iron-sulfur proteins as donors.   \n",
       "24              Acting on the CH-CH group of donors.   \n",
       "1               Acting on the CH-CH group of donors.   \n",
       "35  Acting on other nitrogenous compounds as donors.   \n",
       "2               Acting on the CH-CH group of donors.   \n",
       "\n",
       "                                 corrosion_mechanisms  score_combined Sites  \n",
       "20  ['h2_consumption', 'direct_eet', 'sulfur_metab...       53.040891   NaN  \n",
       "24  ['h2_consumption', 'direct_eet', 'sulfur_metab...       51.298188   NaN  \n",
       "1   ['h2_consumption', 'direct_eet', 'sulfur_metab...       50.718367   NaN  \n",
       "35  ['h2_consumption', 'direct_eet', 'nitrogen_met...       50.402663   NaN  \n",
       "2   ['h2_consumption', 'direct_eet', 'sulfur_metab...       50.379373   NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fully_exploded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66a00f7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sites'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_5883/1022564342.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_combined_features = pd.merge(micro_markers, physichem_markers, on = \u001b[33m\"Sites\"\u001b[39m)\n\u001b[32m      2\u001b[39m corr = df_combined_features.corr()[top_features].loc[\u001b[33m\"O2_Eh\"\u001b[39m, \u001b[33m\"pH_PO4\"\u001b[39m, \u001b[33m\"Fe_Zn_Ox\"\u001b[39m]\n",
      "\u001b[32m~/MIC/3_combined/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32m~/MIC/3_combined/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m~/MIC/3_combined/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1306\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1307\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1311\u001b[39m                         join_names.append(lk)\n\u001b[32m   1312\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32m~/MIC/3_combined/.venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'Sites'"
     ]
    }
   ],
   "source": [
    "df_combined_features = pd.merge(micro_markers, physichem_markers, on = \"Sites\")\n",
    "corr = df_combined_features.corr()[top_features].loc[\"O2_Eh\", \"pH_PO4\", \"Fe_Zn_Ox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4850f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "I have a problem to solve I have a data with only 66 points and the identifier is named phyichem_df[\"Sites\"] the parallel study identified protein_names which are related to \"Genus\" micro_df, and are like [\"protein_name]: [list of genus] or it could be just the pair as well for each protein name a genus, which means same protein name for several genus, that is micro_df. I can actually get the \"Sites\" as well from this micro_df, but the thing is that each site can have same protein-genus pair... so this could be an approach to join the two df and I am no sure wether if just make my small one get inserted on my big one which will produce large amount of samples or to get the small one physichem_df and insert the protein-genus pairs do you understand the problem? at the moment i got no \"Sites\" on micro_df it is a matter of inputing them in the previous function that produce them or I could use a dictionary that has Sites:Genus\n",
    "                                                                                                                                                                                                              \n",
    "This is my micro_df  columns to serve as join the actual df is bigger                                                                                                                                                                                                                                                      protein_name  \\\n",
    "0                 3-oxoacyl-[ reductase (ec 1.1.1.100)   \n",
    "1                     enoyl-[ reductase [ (ec 1.3.1.9)   \n",
    "2    enoyl-[ reductase (nadph, si-specific); acyl-a...   \n",
    "3    beta-ketoacyl-[ synthase iii (beta-ketoacyl-ac...   \n",
    "4    glutathione hydrolase proenzyme (ec 2.3.2.2) 3...   \n",
    "..                                                 ...   \n",
    "96   medium-chain acyl-coa ligase; fadk (gene name)...   \n",
    "97    udp-n-acetylglucosamine 4-epimerase (ec 5.1.3.7)   \n",
    "98      maltose 6'-phosphate phosphatase (ec 3.1.3.90)   \n",
    "99   membrane dipeptidase (ec 3.4.13.19) (peptidase...   \n",
    "100  membrane dipeptidase (ec 3.4.13.19) (peptidase...   \n",
    "\n",
    "                                                genera  \n",
    "0    ['Gallionella', 'Acidisoma', 'Mycoplana', 'The...  \n",
    "1    ['Gallionella', 'Acidisoma', 'Mycoplana', 'Ano...  \n",
    "2    ['Gallionella', 'Acidisoma', 'Aestuariimicrobi...  \n",
    "3    ['Acidisoma', 'Thermincola', 'Mycoplana', 'Ano...  \n",
    "4    ['Flavisolibacter', 'Mycoplana', 'Pseudomonas'... \n",
    "      \n",
    "and this is my phychem_df with \"Sites\" and I will have to input the \"Genus\" singular but I think\n",
    "that is no nesary since I have to get them by a dict since the micro_df has the \"Genus\" as a list\n",
    "\n",
    " Sites  Label  Temperature  Type    EC_M     O2_Eh   Ox_Fe_Zn  Cl_SO4_NO3  \\\n",
    "0  site_1      2        23.00     0  690.40 -1.514130 -19.014017  -12.462139   \n",
    "1  site_2      2        22.81     0  477.35 -3.218897 -27.631021  -11.728597   \n",
    "2  site_3      3        18.80     2  651.00 -2.995939 -19.578501  -12.594547   \n",
    "3  site_4      1        13.70     0  270.98 -3.506600 -19.536276  -17.727482   \n",
    "4  site_5      2    \n",
    "The other columns are shown here for ilustration purposes they are no for joining only Sites is to be used, since \n",
    "algorithms sometimes do no work i am worried that you will make it more complicated that actually is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a654e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6e9e3f0",
   "metadata": {},
   "source": [
    "## Data Splitting Strategy\n",
    "\n",
    "To ensure robust feature engineering and prevent data leakage, a portion of the data will be reserved for final model validation. The split must preserve the distribution of key factors affecting corrosion mechanisms:\n",
    "\n",
    "1. **Corrosion Severity Label (Primary)** - Ensures balanced representation of corrosion levels and Essential for model evaluation across all severity classes\n",
    "\n",
    "2. **Material Composition**: Different materials exhibit distinct corrosion mechanisms\n",
    "\n",
    "3. **System Temperature Regime** Hot/Cold/Combined systems affect: Reaction kinetics, oxygen solubility, protective film formation, mineral precipitation tendencies\n",
    "\n",
    "4. **Geographical Location**: Influences water chemistry through<. Different treatment regulations (chlorine vs. non-chlorine), regional geological variations in mineral content, country-specific water quality standards\n",
    "\n",
    "5. **System Age (Secondary)**: Collection period: 2014-2018, while potentially relevant for corrosion progression it is considered less critical due to varying maintenance histories and treatment variations make precise temporal effects difficult to isolate\n",
    "\n",
    "This stratified splitting approach ensures the test set remains representative while maintaining the independence necessary for valid model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_material_group(df, material_column='Material'):\n",
    "    \"\"\"\n",
    "    Groups materials based on cluster analysis findings and material properties.\n",
    "    \n",
    "    Key groupings:\n",
    "    - Steel_group: Combines Stainless_Steel and Steel\n",
    "    - GSP_group: Combines Galvanized_Steel and Galvanized_Steel_Plastic\n",
    "    - Keeps Galvanized_Carbon_Steel separate due to distinct cluster behavior\n",
    "    - Other materials remain separate for individual analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame containing material information\n",
    "    material_column : str, default='Material'\n",
    "        Name of the column containing material names\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with added 'material_group' column\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the material column to avoid modifying the original\n",
    "    materials = df[material_column].copy()\n",
    "    \n",
    "    grouped_materials = []\n",
    "    \n",
    "    for material in materials:\n",
    "        # Standardize material name\n",
    "        material_strip = str(material).strip()\n",
    "        \n",
    "        # Group 1: Combine Stainless Steel and Steel\n",
    "        if material_strip in ['Stainless_Steel', 'Steel']:\n",
    "            grouped_materials.append('Steel_group')\n",
    "        \n",
    "        # Group 2: Combine Galvanized Steel and GSP\n",
    "        elif material_strip in ['Galvanized_Steel', 'Galvanized_Steel_Plastic']:\n",
    "            grouped_materials.append('GS_group')\n",
    "        \n",
    "        # Keep Galvanized Carbon Steel separate\n",
    "        elif material_strip in ['Galvanized_Carbon_Steel', 'Galvanized_carbon_steel_plastic']:\n",
    "            grouped_materials.append('GCS_group')\n",
    "    \n",
    "        # Keep Cooper as is\n",
    "        elif material_strip == 'Cooper':\n",
    "            grouped_materials.append('Cooper_group')\n",
    "        \n",
    "        # Keep Cross-Linked Polyethylene and THSP as others\n",
    "        elif material_strip in ['Cross-Linked_Polyethylene','Tinplate_High-Strength_Plastic']:\n",
    "            grouped_materials.append('CLP_THSP_group')\n",
    "           \n",
    "    # Add the grouped materials as a new column\n",
    "    df['material_group'] = grouped_materials\n",
    "    return df\n",
    "\n",
    "#the indexes will be kept same as the whole df\n",
    "def create_stratification_groups_v2(row):\n",
    "    \"\"\"\n",
    "    Create comprehensive stratification groups\n",
    "    \"\"\"\n",
    "    # Water treatment regime based on country\n",
    "    water_regime = 'chlorine' if row['Country'] in ['Belgium', 'Netherlands'] else 'no_chlorine' \n",
    "    \n",
    "    # Create stratification group string\n",
    "    strat_group = f\"{row['material_group']}_{water_regime}_label{row['Label']}\" # _{row['Type']}_ I removed the Type condition because it divides the samples in too many classes. \n",
    "    \n",
    "    return strat_group  \n",
    "\n",
    "def split_dataset_v2(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset while maintaining distributions of key features and original indeces\n",
    "    \"\"\"\n",
    "    #Creating a split column with default value, so the spliting can be applied to other dataframe\n",
    "    df['split'] = 'train' # as default value\n",
    "    \n",
    "    # Add material group column\n",
    "    df = get_material_group(df)\n",
    "    #reindexing \n",
    "    df.index = original_indices\n",
    "    # Create stratification groups\n",
    "    df['strat_group'] = df.apply(create_stratification_groups_v2, axis=1)\n",
    "    \n",
    "    # Identify groups with sufficient samples\n",
    "    group_counts = df['strat_group'].value_counts()\n",
    "    large_groups = group_counts[group_counts >= 4].index\n",
    "    small_groups = group_counts[group_counts < 4].index\n",
    "    \n",
    "    # Split data based on group size\n",
    "    large_data = df[df['strat_group'].isin(large_groups)]\n",
    "    small_data = df[df['strat_group'].isin(small_groups)]\n",
    "    \n",
    "    if len(large_data) > 0:\n",
    "        # Stratified split for large groups\n",
    "        train_large_idx, test_large_idx = train_test_split(\n",
    "            large_data.index,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=large_data['strat_group']\n",
    "        )\n",
    "        \n",
    "        if len(small_data) > 0:\n",
    "            train_small_idx, test_small_idx = train_test_split(\n",
    "                small_data.index,\n",
    "                test_size=test_size,\n",
    "                random_state=random_state,\n",
    "                stratify=small_data['strat_group']\n",
    "            )\n",
    "            # Combine indices\n",
    "            train_idx = np.concatenate([train_large_idx, train_small_idx])\n",
    "            test_idx = np.concatenate([test_large_idx, test_small_idx])\n",
    "            \n",
    "        else:  # This else belongs to the small_data if\n",
    "            train_idx = train_large_idx\n",
    "            test_idx = test_large_idx\n",
    "    else:  # This else belongs to the large_data if\n",
    "        # Simple split if no large groups\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            df.index,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    \n",
    "    # Mark split in original dataframe\n",
    "    df.loc[test_idx, 'split'] = 'test'\n",
    "    \n",
    "    # Return train and test dataframes with original indices\n",
    "    return df[df['split'] == 'train'], df[df['split'] == 'test']\n",
    "    \n",
    "def analyze_split_results(train_df, test_df, original_df):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of features in the split datasets\n",
    "    \"\"\"\n",
    "    print(\"=== Split Size Analysis ===\")\n",
    "    print(f\"Total samples: {len(original_df)}\")\n",
    "    print(f\"Training samples: {len(train_df)} ({len(train_df)/len(original_df)*100:.1f}%)\")\n",
    "    print(f\"Test samples: {len(test_df)} ({len(test_df)/len(original_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze distributions\n",
    "    features = ['Material', 'Country', 'Label']\n",
    "    \n",
    "    for feature in features:\n",
    "        print(f\"\\n=== {feature} Distribution ===\")\n",
    "        \n",
    "        # Calculate distributions\n",
    "        train_dist = train_df[feature].value_counts(normalize=True)\n",
    "        test_dist = test_df[feature].value_counts(normalize=True)\n",
    "        original_dist = original_df[feature].value_counts(normalize=True)\n",
    "        \n",
    "        # Combine into a DataFrame\n",
    "        dist_df = pd.DataFrame({\n",
    "            'Original %': original_dist * 100,\n",
    "            'Train %': train_dist * 100,\n",
    "            'Test %': test_dist * 100,\n",
    "            'Original Count': original_df[feature].value_counts(),\n",
    "            'Train Count': train_df[feature].value_counts(),\n",
    "            'Test Count': test_df[feature].value_counts()\n",
    "        }).round(2)\n",
    "        \n",
    "        print(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d6404",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_Meta_Split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m grouped_materials_df = get_material_group(\u001b[43mdf_Meta_Split\u001b[49m, material_column=\u001b[33m'\u001b[39m\u001b[33mMaterial\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Apply the split\u001b[39;00m\n\u001b[32m      3\u001b[39m train_df, test_df = split_dataset_v2(grouped_materials_df)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_Meta_Split' is not defined"
     ]
    }
   ],
   "source": [
    "grouped_materials_df = get_material_group(df_Meta_Split, material_column='Material')\n",
    "# Apply the split\n",
    "train_df, test_df = split_dataset_v2(grouped_materials_df)\n",
    "# Analyzing the results\n",
    "analyze_split_results(train_df, test_df, grouped_materials_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c06d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now applying the split to original dataframe\n",
    "split_mapping = grouped_materials_df['split']\n",
    "original['split'] = original.index.map(split_mapping)\n",
    "\n",
    "# Get train and test sets for original dataframe\n",
    "original_train = original[original['split'] == 'train']\n",
    "original_test = original[original['split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify indices match\n",
    "print(\"Train indices match:\", set(train_df.index) == set(original_train.index))\n",
    "print(\"Test indices match:\", set(test_df.index) == set(original_test.index))\n",
    "print(\"No overlap between train and test:\", len(set(train_df.index) & set(test_df.index)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we assigne the original_train set to the dataframe to work in this notebook moving forward and we keep the original_test df to work for the model validation\n",
    "df = original_train.drop(columns=['split']).copy() # .drop(columns=['Label']).values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a5d80",
   "metadata": {},
   "source": [
    "The initial strategy for dataset partitioning aimed to ensure a balanced representation across Location, Material, Type, and Label variables. Despite implementing sophisticated preprocessing steps - including hierarchical cluster analysis for material grouping and stratification of locations based on Cl- treatment protocols - the limited sample size (n=13) proved insufficient relative to the number of distinct classes, preventing a statistically valid split.\n",
    "\n",
    "To address this limitation, Type was removed from the stratification criteria. This decision was supported by two key analytical findings:\n",
    "\n",
    "Principal Component Analysis (PCA) in three dimensions demonstrated strong clustering patterns based on materials and locations alone, suggesting these features effectively capture the underlying data structure.\n",
    "\n",
    "Feature importance analysis using XGBoost revealed that Composition accounts for approximately 50% of the variance in material type distribution across clusters, indicating that material properties are inherently captured through compositional data.\n",
    "\n",
    "This modification to the stratification approach maintains the essential patterns in the data while enabling a more robust train-test split for subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel = df[top_features]\n",
    "y = df['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # or softmax for >2 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cef101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.12/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_37861/169521571.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Data preparation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X_sel = \u001b[43mdf\u001b[49m[top_features]\n\u001b[32m     10\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mCategory\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data preparation\n",
    "X_sel = df[top_features]\n",
    "y = df['Category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values)\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 32)\n",
    "        self.layer2 = nn.Linear(32, 16)\n",
    "        self.layer3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "model = SimpleNN(input_size)\n",
    "print(f\"Model created with input size: {input_size}\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with smaller batches\n",
    "num_epochs = 10\n",
    "batch_size = 8  # Small batch size for memory efficiency\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Use smaller batches to reduce memory usage\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Get batch\n",
    "        batch_X = X_train_tensor[i:i+batch_size]\n",
    "        batch_y = y_train_tensor[i:i+batch_size].view(-1, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / (len(X_train) / batch_size)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Evaluate every epoch to track progress\n",
    "    if (epoch + 1) % 2 == 0:  # Check every 2 epochs\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test_tensor)\n",
    "            y_pred_class = (y_pred > 0.5).float()\n",
    "            accuracy = (y_pred_class.view(-1) == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "            print(f'Validation Accuracy after epoch {epoch+1}: {accuracy:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_class = (y_pred > 0.5).float()\n",
    "    accuracy = (y_pred_class.view(-1) == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "    print(f'Final Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Print predictions for first few samples\n",
    "    for i in range(min(5, len(y_test))):\n",
    "        print(f\"Sample {i+1}: Actual: {y_test.iloc[i]}, Predicted: {y_pred[i].item():.4f}, Class: {y_pred_class[i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3_Combined Environment",
   "language": "python",
   "name": "combined_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
