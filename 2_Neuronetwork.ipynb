{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f36d6a4",
   "metadata": {},
   "source": [
    "df_physicochemical comprises the selected features phychem_features and the whole set of parameters if necesary for plotting purposes.\n",
    "The df_micro is the df of protein-genus features selected from the notebook 7_visual_proteins_ipnyb\n",
    "micro_usuals is a dictionary with the list of proven bacteria influencing corrosion and could serve as label for plotting purposes\n",
    "micro_markers is the dictionary with the list of bacteria belonging to the df_micro dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57d8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c19f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Local\n",
    "base_dir = Path(\"/home/beatriz/MIC/3_combined/data\")\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "markers_path = base_dir /\"combined_markers.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf64e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local (VSCode) environment\n"
     ]
    }
   ],
   "source": [
    "# Deciding which environment\n",
    "if Path(\"/kaggle\").exists():\n",
    "    print(\"Running in Kaggle environment\")\n",
    "    # For Kaggle work# Input datasets (read-only in Kaggle) # Files in small input directory\n",
    "    base_dir = Path(\"/kaggle/input\")  \n",
    "    abundance_excel = base_dir / \"new-picrust/merged_to_sequence.xlsx\" # inside input small sizes input\n",
    "    #Input market groups\n",
    "    market_dir = base_dir / \"markers\"\n",
    "    # Output dirs\n",
    "    output_base = Path(\"/kaggle/working/\")\n",
    "    # Save the present working data\n",
    "    combined_path = output_base /\"combined_markers.xlsx\"\n",
    "    #Directory to keep  Results\n",
    "    share_dir = output_base/\"Visualisations\"\n",
    "    shared_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "else:\n",
    "    print(\"Running in local (VSCode) environment\")\n",
    "    base_dir = Path(\"data\")\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Base Paths for local environment\n",
    "    abundance_excel = base_dir / \"merged_to_sequence.xlsx\"\n",
    "    # This files are too large for github and are store on Kaggle for educational purposes\n",
    "    output_large = Path(\"/home/beatriz/MIC/output_large\")\n",
    "    #Input market groups\n",
    "    market_dir = output_large / \"markers.parquet/\"  # Directory\n",
    "    output_base = base_dir \n",
    "    #Directory to keep some Results\n",
    "    shared_dir= Path(\"/home/beatriz/SharedFolder/Visualisations/\")\n",
    "    combined_path = base_dir / \"combined_markers.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc808b5",
   "metadata": {},
   "source": [
    "### Importing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baf81ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_markers = pd.read_excel(markers_path, sheet_name='protein_markers',  engine ='openpyxl')\n",
    "all_physichem = pd.read_excel(markers_path, sheet_name='all_physicochemical', engine ='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31bd6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "physichem_features = ['Sites', 'Label', 'Temperature', 'Type', 'EC_M', 'O2_Eh',\n",
    "                     'Ox_Fe_Zn', 'Cl_SO4_NO3', 'Na_K','pH_HPO4',\n",
    "                       'Ca_HCO3_Mg', 'Cu_Al_Mn', 'Ni_Cr_Mo']\n",
    "physichem_markers = all_physichem[physichem_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bff05e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sites</th>\n",
       "      <th>Label</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Type</th>\n",
       "      <th>EC_M</th>\n",
       "      <th>O2_Eh</th>\n",
       "      <th>Ox_Fe_Zn</th>\n",
       "      <th>Cl_SO4_NO3</th>\n",
       "      <th>Na_K</th>\n",
       "      <th>pH_HPO4</th>\n",
       "      <th>Ca_HCO3_Mg</th>\n",
       "      <th>Cu_Al_Mn</th>\n",
       "      <th>Ni_Cr_Mo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>site_1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0</td>\n",
       "      <td>690.40</td>\n",
       "      <td>-1.514130</td>\n",
       "      <td>-19.014017</td>\n",
       "      <td>-12.462139</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-15.203948</td>\n",
       "      <td>-22.581420</td>\n",
       "      <td>-27.631021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>site_2</td>\n",
       "      <td>2</td>\n",
       "      <td>22.81</td>\n",
       "      <td>0</td>\n",
       "      <td>477.35</td>\n",
       "      <td>-3.218897</td>\n",
       "      <td>-27.631021</td>\n",
       "      <td>-11.728597</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-17.562225</td>\n",
       "      <td>-27.532081</td>\n",
       "      <td>-27.631021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>site_3</td>\n",
       "      <td>3</td>\n",
       "      <td>18.80</td>\n",
       "      <td>2</td>\n",
       "      <td>651.00</td>\n",
       "      <td>-2.995939</td>\n",
       "      <td>-19.578501</td>\n",
       "      <td>-12.594547</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-14.671984</td>\n",
       "      <td>-22.128057</td>\n",
       "      <td>-27.631021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>site_4</td>\n",
       "      <td>1</td>\n",
       "      <td>13.70</td>\n",
       "      <td>0</td>\n",
       "      <td>270.98</td>\n",
       "      <td>-3.506600</td>\n",
       "      <td>-19.536276</td>\n",
       "      <td>-17.727482</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>2.704164e-10</td>\n",
       "      <td>-17.487376</td>\n",
       "      <td>-25.042671</td>\n",
       "      <td>-27.631021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>site_5</td>\n",
       "      <td>2</td>\n",
       "      <td>18.81</td>\n",
       "      <td>0</td>\n",
       "      <td>282.45</td>\n",
       "      <td>-1.272968</td>\n",
       "      <td>-22.072423</td>\n",
       "      <td>-17.727483</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>1.163699e-10</td>\n",
       "      <td>-17.516815</td>\n",
       "      <td>-27.631021</td>\n",
       "      <td>-27.631021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sites  Label  Temperature  Type    EC_M     O2_Eh   Ox_Fe_Zn  Cl_SO4_NO3  \\\n",
       "0  site_1      2        23.00     0  690.40 -1.514130 -19.014017  -12.462139   \n",
       "1  site_2      2        22.81     0  477.35 -3.218897 -27.631021  -11.728597   \n",
       "2  site_3      3        18.80     2  651.00 -2.995939 -19.578501  -12.594547   \n",
       "3  site_4      1        13.70     0  270.98 -3.506600 -19.536276  -17.727482   \n",
       "4  site_5      2        18.81     0  282.45 -1.272968 -22.072423  -17.727483   \n",
       "\n",
       "       Na_K       pH_HPO4  Ca_HCO3_Mg   Cu_Al_Mn   Ni_Cr_Mo  \n",
       "0  0.002190  0.000000e+00  -15.203948 -22.581420 -27.631021  \n",
       "1  0.002450  0.000000e+00  -17.562225 -27.532081 -27.631021  \n",
       "2  0.002128  0.000000e+00  -14.671984 -22.128057 -27.631021  \n",
       "3  0.001031  2.704164e-10  -17.487376 -25.042671 -27.631021  \n",
       "4  0.000823  1.163699e-10  -17.516815 -27.631021 -27.631021  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physichem_markers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c6166b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>protein_name</th>\n",
       "      <th>genera_count</th>\n",
       "      <th>genera</th>\n",
       "      <th>functional_categories</th>\n",
       "      <th>niche_pathways</th>\n",
       "      <th>enzyme_class</th>\n",
       "      <th>corrosion_mechanisms</th>\n",
       "      <th>score_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3-oxoacyl-[ reductase (ec 1.1.1.100)</td>\n",
       "      <td>13</td>\n",
       "      <td>['Gallionella', 'Acidisoma', 'Mycoplana', 'The...</td>\n",
       "      <td>['h2_consumption', 'iron/sulfur_redox', 'acid_...</td>\n",
       "      <td>['biotin', 'fatty acid biosynthesis', 'lipid b...</td>\n",
       "      <td>Acting on the CH-OH group of donors.</td>\n",
       "      <td>['h2_consumption', 'direct_eet', 'sulfur_metab...</td>\n",
       "      <td>50.329164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>enoyl-[ reductase [ (ec 1.3.1.9)</td>\n",
       "      <td>11</td>\n",
       "      <td>['Gallionella', 'Acidisoma', 'Mycoplana', 'Ano...</td>\n",
       "      <td>['h2_consumption', 'iron/sulfur_redox', 'acid_...</td>\n",
       "      <td>['lipid biosynthesis proteins', 'fatty acid bi...</td>\n",
       "      <td>Acting on the CH-CH group of donors.</td>\n",
       "      <td>['h2_consumption', 'direct_eet', 'sulfur_metab...</td>\n",
       "      <td>50.718367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>enoyl-[ reductase (nadph, si-specific); acyl-a...</td>\n",
       "      <td>10</td>\n",
       "      <td>['Gallionella', 'Acidisoma', 'Aestuariimicrobi...</td>\n",
       "      <td>['h2_consumption', 'iron/sulfur_redox', 'acid_...</td>\n",
       "      <td>['biotin', 'fatty acid biosynthesis', 'lipid b...</td>\n",
       "      <td>Acting on the CH-CH group of donors.</td>\n",
       "      <td>['h2_consumption', 'direct_eet', 'sulfur_metab...</td>\n",
       "      <td>50.379373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>beta-ketoacyl-[ synthase iii (beta-ketoacyl-ac...</td>\n",
       "      <td>9</td>\n",
       "      <td>['Acidisoma', 'Thermincola', 'Mycoplana', 'Ano...</td>\n",
       "      <td>['h2_consumption', 'iron/sulfur_redox', 'acid_...</td>\n",
       "      <td>['nitrogen , nitrogen cycle']</td>\n",
       "      <td>Acting on other nitrogenous compounds as donors.</td>\n",
       "      <td>['h2_consumption', 'direct_eet', 'nitrogen_met...</td>\n",
       "      <td>49.663520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>glutathione hydrolase proenzyme (ec 2.3.2.2) 3...</td>\n",
       "      <td>8</td>\n",
       "      <td>['Flavisolibacter', 'Mycoplana', 'Pseudomonas'...</td>\n",
       "      <td>['h2_consumption', 'iron/sulfur_redox', 'acid_...</td>\n",
       "      <td>['nitrogen , nitrogen cycle']</td>\n",
       "      <td>Acting on other nitrogenous compounds as donors.</td>\n",
       "      <td>['h2_consumption', 'direct_eet', 'nitrogen_met...</td>\n",
       "      <td>49.825548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       protein_name  \\\n",
       "0           0               3-oxoacyl-[ reductase (ec 1.1.1.100)   \n",
       "1           1                   enoyl-[ reductase [ (ec 1.3.1.9)   \n",
       "2           2  enoyl-[ reductase (nadph, si-specific); acyl-a...   \n",
       "3           3  beta-ketoacyl-[ synthase iii (beta-ketoacyl-ac...   \n",
       "4           4  glutathione hydrolase proenzyme (ec 2.3.2.2) 3...   \n",
       "\n",
       "   genera_count                                             genera  \\\n",
       "0            13  ['Gallionella', 'Acidisoma', 'Mycoplana', 'The...   \n",
       "1            11  ['Gallionella', 'Acidisoma', 'Mycoplana', 'Ano...   \n",
       "2            10  ['Gallionella', 'Acidisoma', 'Aestuariimicrobi...   \n",
       "3             9  ['Acidisoma', 'Thermincola', 'Mycoplana', 'Ano...   \n",
       "4             8  ['Flavisolibacter', 'Mycoplana', 'Pseudomonas'...   \n",
       "\n",
       "                               functional_categories  \\\n",
       "0  ['h2_consumption', 'iron/sulfur_redox', 'acid_...   \n",
       "1  ['h2_consumption', 'iron/sulfur_redox', 'acid_...   \n",
       "2  ['h2_consumption', 'iron/sulfur_redox', 'acid_...   \n",
       "3  ['h2_consumption', 'iron/sulfur_redox', 'acid_...   \n",
       "4  ['h2_consumption', 'iron/sulfur_redox', 'acid_...   \n",
       "\n",
       "                                      niche_pathways  \\\n",
       "0  ['biotin', 'fatty acid biosynthesis', 'lipid b...   \n",
       "1  ['lipid biosynthesis proteins', 'fatty acid bi...   \n",
       "2  ['biotin', 'fatty acid biosynthesis', 'lipid b...   \n",
       "3                      ['nitrogen , nitrogen cycle']   \n",
       "4                      ['nitrogen , nitrogen cycle']   \n",
       "\n",
       "                                       enzyme_class  \\\n",
       "0              Acting on the CH-OH group of donors.   \n",
       "1              Acting on the CH-CH group of donors.   \n",
       "2              Acting on the CH-CH group of donors.   \n",
       "3  Acting on other nitrogenous compounds as donors.   \n",
       "4  Acting on other nitrogenous compounds as donors.   \n",
       "\n",
       "                                corrosion_mechanisms  score_combined  \n",
       "0  ['h2_consumption', 'direct_eet', 'sulfur_metab...       50.329164  \n",
       "1  ['h2_consumption', 'direct_eet', 'sulfur_metab...       50.718367  \n",
       "2  ['h2_consumption', 'direct_eet', 'sulfur_metab...       50.379373  \n",
       "3  ['h2_consumption', 'direct_eet', 'nitrogen_met...       49.663520  \n",
       "4  ['h2_consumption', 'direct_eet', 'nitrogen_met...       49.825548  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_markers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "913c83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_markerstop = micro_markers.sort_values(by='score_combined', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a00f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_features = pd.merge(micro_markers, physichem_markers, on = \"Sites\")\n",
    "corr = df_combined_features.corr()[top_features].loc[\"O2_Eh\", \"pH_PO4\", \"Fe_Zn_Ox\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a654e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6e9e3f0",
   "metadata": {},
   "source": [
    "## Data Splitting Strategy\n",
    "\n",
    "To ensure robust feature engineering and prevent data leakage, a portion of the data will be reserved for final model validation. The split must preserve the distribution of key factors affecting corrosion mechanisms:\n",
    "\n",
    "1. **Corrosion Severity Label (Primary)** - Ensures balanced representation of corrosion levels and Essential for model evaluation across all severity classes\n",
    "\n",
    "2. **Material Composition**: Different materials exhibit distinct corrosion mechanisms\n",
    "\n",
    "3. **System Temperature Regime** Hot/Cold/Combined systems affect: Reaction kinetics, oxygen solubility, protective film formation, mineral precipitation tendencies\n",
    "\n",
    "4. **Geographical Location**: Influences water chemistry through<. Different treatment regulations (chlorine vs. non-chlorine), regional geological variations in mineral content, country-specific water quality standards\n",
    "\n",
    "5. **System Age (Secondary)**: Collection period: 2014-2018, while potentially relevant for corrosion progression it is considered less critical due to varying maintenance histories and treatment variations make precise temporal effects difficult to isolate\n",
    "\n",
    "This stratified splitting approach ensures the test set remains representative while maintaining the independence necessary for valid model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_material_group(df, material_column='Material'):\n",
    "    \"\"\"\n",
    "    Groups materials based on cluster analysis findings and material properties.\n",
    "    \n",
    "    Key groupings:\n",
    "    - Steel_group: Combines Stainless_Steel and Steel\n",
    "    - GSP_group: Combines Galvanized_Steel and Galvanized_Steel_Plastic\n",
    "    - Keeps Galvanized_Carbon_Steel separate due to distinct cluster behavior\n",
    "    - Other materials remain separate for individual analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame containing material information\n",
    "    material_column : str, default='Material'\n",
    "        Name of the column containing material names\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with added 'material_group' column\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the material column to avoid modifying the original\n",
    "    materials = df[material_column].copy()\n",
    "    \n",
    "    grouped_materials = []\n",
    "    \n",
    "    for material in materials:\n",
    "        # Standardize material name\n",
    "        material_strip = str(material).strip()\n",
    "        \n",
    "        # Group 1: Combine Stainless Steel and Steel\n",
    "        if material_strip in ['Stainless_Steel', 'Steel']:\n",
    "            grouped_materials.append('Steel_group')\n",
    "        \n",
    "        # Group 2: Combine Galvanized Steel and GSP\n",
    "        elif material_strip in ['Galvanized_Steel', 'Galvanized_Steel_Plastic']:\n",
    "            grouped_materials.append('GS_group')\n",
    "        \n",
    "        # Keep Galvanized Carbon Steel separate\n",
    "        elif material_strip in ['Galvanized_Carbon_Steel', 'Galvanized_carbon_steel_plastic']:\n",
    "            grouped_materials.append('GCS_group')\n",
    "    \n",
    "        # Keep Cooper as is\n",
    "        elif material_strip == 'Cooper':\n",
    "            grouped_materials.append('Cooper_group')\n",
    "        \n",
    "        # Keep Cross-Linked Polyethylene and THSP as others\n",
    "        elif material_strip in ['Cross-Linked_Polyethylene','Tinplate_High-Strength_Plastic']:\n",
    "            grouped_materials.append('CLP_THSP_group')\n",
    "           \n",
    "    # Add the grouped materials as a new column\n",
    "    df['material_group'] = grouped_materials\n",
    "    return df\n",
    "\n",
    "#the indexes will be kept same as the whole df\n",
    "def create_stratification_groups_v2(row):\n",
    "    \"\"\"\n",
    "    Create comprehensive stratification groups\n",
    "    \"\"\"\n",
    "    # Water treatment regime based on country\n",
    "    water_regime = 'chlorine' if row['Country'] in ['Belgium', 'Netherlands'] else 'no_chlorine' \n",
    "    \n",
    "    # Create stratification group string\n",
    "    strat_group = f\"{row['material_group']}_{water_regime}_label{row['Label']}\" # _{row['Type']}_ I removed the Type condition because it divides the samples in too many classes. \n",
    "    \n",
    "    return strat_group  \n",
    "\n",
    "def split_dataset_v2(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset while maintaining distributions of key features and original indeces\n",
    "    \"\"\"\n",
    "    #Creating a split column with default value, so the spliting can be applied to other dataframe\n",
    "    df['split'] = 'train' # as default value\n",
    "    \n",
    "    # Add material group column\n",
    "    df = get_material_group(df)\n",
    "    #reindexing \n",
    "    df.index = original_indices\n",
    "    # Create stratification groups\n",
    "    df['strat_group'] = df.apply(create_stratification_groups_v2, axis=1)\n",
    "    \n",
    "    # Identify groups with sufficient samples\n",
    "    group_counts = df['strat_group'].value_counts()\n",
    "    large_groups = group_counts[group_counts >= 4].index\n",
    "    small_groups = group_counts[group_counts < 4].index\n",
    "    \n",
    "    # Split data based on group size\n",
    "    large_data = df[df['strat_group'].isin(large_groups)]\n",
    "    small_data = df[df['strat_group'].isin(small_groups)]\n",
    "    \n",
    "    if len(large_data) > 0:\n",
    "        # Stratified split for large groups\n",
    "        train_large_idx, test_large_idx = train_test_split(\n",
    "            large_data.index,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=large_data['strat_group']\n",
    "        )\n",
    "        \n",
    "        if len(small_data) > 0:\n",
    "            train_small_idx, test_small_idx = train_test_split(\n",
    "                small_data.index,\n",
    "                test_size=test_size,\n",
    "                random_state=random_state,\n",
    "                stratify=small_data['strat_group']\n",
    "            )\n",
    "            # Combine indices\n",
    "            train_idx = np.concatenate([train_large_idx, train_small_idx])\n",
    "            test_idx = np.concatenate([test_large_idx, test_small_idx])\n",
    "            \n",
    "        else:  # This else belongs to the small_data if\n",
    "            train_idx = train_large_idx\n",
    "            test_idx = test_large_idx\n",
    "    else:  # This else belongs to the large_data if\n",
    "        # Simple split if no large groups\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            df.index,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    \n",
    "    # Mark split in original dataframe\n",
    "    df.loc[test_idx, 'split'] = 'test'\n",
    "    \n",
    "    # Return train and test dataframes with original indices\n",
    "    return df[df['split'] == 'train'], df[df['split'] == 'test']\n",
    "    \n",
    "def analyze_split_results(train_df, test_df, original_df):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of features in the split datasets\n",
    "    \"\"\"\n",
    "    print(\"=== Split Size Analysis ===\")\n",
    "    print(f\"Total samples: {len(original_df)}\")\n",
    "    print(f\"Training samples: {len(train_df)} ({len(train_df)/len(original_df)*100:.1f}%)\")\n",
    "    print(f\"Test samples: {len(test_df)} ({len(test_df)/len(original_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze distributions\n",
    "    features = ['Material', 'Country', 'Label']\n",
    "    \n",
    "    for feature in features:\n",
    "        print(f\"\\n=== {feature} Distribution ===\")\n",
    "        \n",
    "        # Calculate distributions\n",
    "        train_dist = train_df[feature].value_counts(normalize=True)\n",
    "        test_dist = test_df[feature].value_counts(normalize=True)\n",
    "        original_dist = original_df[feature].value_counts(normalize=True)\n",
    "        \n",
    "        # Combine into a DataFrame\n",
    "        dist_df = pd.DataFrame({\n",
    "            'Original %': original_dist * 100,\n",
    "            'Train %': train_dist * 100,\n",
    "            'Test %': test_dist * 100,\n",
    "            'Original Count': original_df[feature].value_counts(),\n",
    "            'Train Count': train_df[feature].value_counts(),\n",
    "            'Test Count': test_df[feature].value_counts()\n",
    "        }).round(2)\n",
    "        \n",
    "        print(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d6404",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_Meta_Split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m grouped_materials_df = get_material_group(\u001b[43mdf_Meta_Split\u001b[49m, material_column=\u001b[33m'\u001b[39m\u001b[33mMaterial\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Apply the split\u001b[39;00m\n\u001b[32m      3\u001b[39m train_df, test_df = split_dataset_v2(grouped_materials_df)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_Meta_Split' is not defined"
     ]
    }
   ],
   "source": [
    "grouped_materials_df = get_material_group(df_Meta_Split, material_column='Material')\n",
    "# Apply the split\n",
    "train_df, test_df = split_dataset_v2(grouped_materials_df)\n",
    "# Analyzing the results\n",
    "analyze_split_results(train_df, test_df, grouped_materials_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c06d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now applying the split to original dataframe\n",
    "split_mapping = grouped_materials_df['split']\n",
    "original['split'] = original.index.map(split_mapping)\n",
    "\n",
    "# Get train and test sets for original dataframe\n",
    "original_train = original[original['split'] == 'train']\n",
    "original_test = original[original['split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify indices match\n",
    "print(\"Train indices match:\", set(train_df.index) == set(original_train.index))\n",
    "print(\"Test indices match:\", set(test_df.index) == set(original_test.index))\n",
    "print(\"No overlap between train and test:\", len(set(train_df.index) & set(test_df.index)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we assigne the original_train set to the dataframe to work in this notebook moving forward and we keep the original_test df to work for the model validation\n",
    "df = original_train.drop(columns=['split']).copy() # .drop(columns=['Label']).values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a5d80",
   "metadata": {},
   "source": [
    "The initial strategy for dataset partitioning aimed to ensure a balanced representation across Location, Material, Type, and Label variables. Despite implementing sophisticated preprocessing steps - including hierarchical cluster analysis for material grouping and stratification of locations based on Cl- treatment protocols - the limited sample size (n=13) proved insufficient relative to the number of distinct classes, preventing a statistically valid split.\n",
    "\n",
    "To address this limitation, Type was removed from the stratification criteria. This decision was supported by two key analytical findings:\n",
    "\n",
    "Principal Component Analysis (PCA) in three dimensions demonstrated strong clustering patterns based on materials and locations alone, suggesting these features effectively capture the underlying data structure.\n",
    "\n",
    "Feature importance analysis using XGBoost revealed that Composition accounts for approximately 50% of the variance in material type distribution across clusters, indicating that material properties are inherently captured through compositional data.\n",
    "\n",
    "This modification to the stratification approach maintains the essential patterns in the data while enabling a more robust train-test split for subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel = df[top_features]\n",
    "y = df['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # or softmax for >2 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cef101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.12/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_37861/169521571.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/beatriz/MIC/3_combined/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Data preparation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X_sel = \u001b[43mdf\u001b[49m[top_features]\n\u001b[32m     10\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mCategory\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data preparation\n",
    "X_sel = df[top_features]\n",
    "y = df['Category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values)\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 32)\n",
    "        self.layer2 = nn.Linear(32, 16)\n",
    "        self.layer3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "model = SimpleNN(input_size)\n",
    "print(f\"Model created with input size: {input_size}\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with smaller batches\n",
    "num_epochs = 10\n",
    "batch_size = 8  # Small batch size for memory efficiency\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Use smaller batches to reduce memory usage\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Get batch\n",
    "        batch_X = X_train_tensor[i:i+batch_size]\n",
    "        batch_y = y_train_tensor[i:i+batch_size].view(-1, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / (len(X_train) / batch_size)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Evaluate every epoch to track progress\n",
    "    if (epoch + 1) % 2 == 0:  # Check every 2 epochs\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test_tensor)\n",
    "            y_pred_class = (y_pred > 0.5).float()\n",
    "            accuracy = (y_pred_class.view(-1) == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "            print(f'Validation Accuracy after epoch {epoch+1}: {accuracy:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_class = (y_pred > 0.5).float()\n",
    "    accuracy = (y_pred_class.view(-1) == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "    print(f'Final Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Print predictions for first few samples\n",
    "    for i in range(min(5, len(y_test))):\n",
    "        print(f\"Sample {i+1}: Actual: {y_test.iloc[i]}, Predicted: {y_pred[i].item():.4f}, Class: {y_pred_class[i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3_Combined Environment",
   "language": "python",
   "name": "combined_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
